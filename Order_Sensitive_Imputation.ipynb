{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "df = pd.read_excel('xlsx_file_path', header=None)\n",
    "# nan_values = df.isnull().any()\n",
    "# print(nan_values)\n",
    "## drroping the column\n",
    "# df=df[df.columns[df.isna().any()]]\n",
    "## dropping row\n",
    "null_data = df[df.isnull().any(axis=1)]\n",
    "# print(null_data)\n",
    "data_top=null_data.index\n",
    "index=[]\n",
    "for i in data_top:\n",
    "    index.append(i)\n",
    "print(index)\n",
    "df=cdist(df,df,'euclid')\n",
    "df=pd.DataFrame(df)\n",
    "# print(df.to_string())\n",
    "#similarity dataset\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into a numpy array and fixing the threshold\n",
    "a= np.array(df)\n",
    "med = 0.785\n",
    "print(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only values above threshold and making others as 0.\n",
    "similarity_thres = []\n",
    "for i in a:\n",
    "    f1=[]\n",
    "    for k in i:\n",
    "        if(k==1):\n",
    "            f1.append(0)\n",
    "        elif(k>=med):\n",
    "            f1.append(1)\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    similarity_thres.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the boolean matrix\n",
    "# similarity_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe using the boolean matrix\n",
    "new_df = pd.DataFrame(similarity_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an index column to get the label in the desired format.\n",
    "new_df['DOC'] = (new_df.index)\n",
    "new_df['X']= \"X\"\n",
    "new_df['DOC2'] = new_df['X']+new_df['DOC'].astype(str)\n",
    "del new_df['DOC'],new_df['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting the network \n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# stacked = new_df.set_index(['DOC2']).stack()\n",
    "# stacked = stacked[stacked==1]\n",
    "# edges = stacked.index.tolist()\n",
    "# #print(il1)\n",
    "# #edges\n",
    "# G = nx.Graph(edges)\n",
    "# Gp = nx.bipartite.project(G,new_df.set_index('DOC2').columns)\n",
    "# Gp.edges()\n",
    "# #pos = {node:[0, i] for i,node in enumerate(il1['DOC2'])}\n",
    "# #pos.update({node:[1, i] for i,node in enumerate(il1['level_1'])})\n",
    "# nx.draw(G, with_labels=True)\n",
    "# #for p in pos:  # raise text positions\n",
    "#  #   pos[p][1] += .25\n",
    "# #nx.draw_networkx_labels(G,pos=pos)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the original dataset and performing the same to create labels and reindex.\n",
    "dataset = pd.read_excel('xlsx_file_path', header=None)\n",
    "dataset.index = dataset.index+1\n",
    "dataset['DOC'] = (dataset.index)\n",
    "dataset['X']= \"X\"\n",
    "dataset['index'] = dataset['X']+dataset['DOC'].astype(str)\n",
    "del dataset['DOC'],dataset['X']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the null values and creating a new dataframe using the similarity boolean matrix.\n",
    "#dataset.set_index('index',inplace=True)\n",
    "dataset\n",
    "null_data = dataset[dataset.isnull().any(axis=1)]\n",
    "similarity_bool = new_df.copy()\n",
    "similarity_bool.set_index('DOC2',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting the index and deleting the index column\n",
    "similarity_bool.reset_index(inplace=True)\n",
    "#del similarity_bool['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del similarity_bool['level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking only values above threshold but instead of converting it into 0 & 1 , we are keeping the similarity score as such.\n",
    "similarity_thres1 = []\n",
    "for i in a:\n",
    "    f1=[]\n",
    "    for k in i:\n",
    "        if(k==1):\n",
    "            f1.append(0)\n",
    "        elif(k>=med):\n",
    "            f1.append(k)\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    similarity_thres1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sscore is a new dataframe with the above scores.\n",
    "sscore = pd.DataFrame(similarity_thres1)\n",
    "sscore.index = sscore.index+1\n",
    "sscore['DOC'] = (sscore.index)\n",
    "sscore['X']= \"X\"\n",
    "sscore['DOC2'] = sscore['X']+sscore['DOC'].astype(str)\n",
    "del sscore['DOC'],sscore['X']\n",
    "# sscore.set_index('DOC2',inplace=True)\n",
    "# print(sscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing stacked to only take the values not equal to 0.0 and then unstacking it.\n",
    "stacked1 = sscore.set_index(['DOC2']).stack()\n",
    "#print(stacked1)\n",
    "stacked1 = stacked1[stacked1 != 0.0]\n",
    "# print(stacked1)\n",
    "edges = stacked1.index.tolist()\n",
    "# print(edges)\n",
    "unstacked = stacked1.unstack()\n",
    "# print(unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the incomplete_tuples and complete_tuples from the unstacked dataframe.\n",
    "print(unstacked)\n",
    "##To determine the P value\n",
    "incomplete_tuples = unstacked.columns.to_list()\n",
    "total_tuples = unstacked.index.to_list()\n",
    "complete_tuples = list(set(total_tuples) - set(incomplete_tuples))\n",
    "print(incomplete_tuples)\n",
    "print(complete_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column in unstacked to house the index values.\n",
    "unstacked['indexed'] = unstacked.index\n",
    "#filling the unstacked df for NaN\n",
    "unstacked.fillna(0,inplace=True)\n",
    "#del unstacked['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only complete neighbours for the incomplete tuples in this \"inc_complete\" dataframe.\n",
    "inc_complete = unstacked[~unstacked['indexed'].isin(incomplete_tuples)]\n",
    "print(inc_complete)\n",
    "#inc_complete\n",
    "#Keeping only incomplete neighbours for the incomplete tuples in this \"inc_complete\" dataframe.\n",
    "inc_incomplete =unstacked[unstacked['indexed'].isin(incomplete_tuples)]\n",
    "print(inc_incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get complete neighbours and their similarities in a dictionary\n",
    "#iterating over the columns in inc_complete, retrieving the complete neighbours for each incomplete tuple and putting them in a dictionary.\n",
    "def incomplete_connection_creator(list_1,dict_1):\n",
    "  listed = []\n",
    "  listed1= []\n",
    "  inc_complete = list_1\n",
    "  incomplete_tuples = dict_1\n",
    "  for column in inc_complete:\n",
    "    name = column\n",
    "    #print(name)\n",
    "    kailai = (list(inc_complete[inc_complete[column] != 0.0].indexed))\n",
    "    kailai1 = (list(inc_complete[inc_complete[column] != 0.0][column]))\n",
    "    #print(kailai)\n",
    "    listed.append(kailai)\n",
    "    listed1.append(kailai1)\n",
    "  #print(listed)\n",
    "  zipped = zip(incomplete_tuples,listed)\n",
    "  zipped1 = zip(incomplete_tuples,listed1)\n",
    "  dict_da = dict(zipped)\n",
    "  dict_db = dict(zipped1)\n",
    "  return dict_da,dict_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the complete neighbours and their similarity scores\n",
    "data_dict,data_dict1 = incomplete_connection_creator(inc_complete,incomplete_tuples)\n",
    "# print(data_dict,\"\\n\",data_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating the p value for the final calculation of node weights.\n",
    "import math\n",
    "def p_value_calc(list_1,dict_1):\n",
    "  incomplete_tuples = list_1\n",
    "  data_dict = dict_1\n",
    "  ##calculating the term (1/Xm)\n",
    "  if len(incomplete_tuples) != 0:\n",
    "    term = 1/(len(incomplete_tuples))\n",
    "  else:\n",
    "    term = 1\n",
    "  ##Summing all the length of values present in the dictionary to get the total number of complete neighbours to the incomplete tuples.\n",
    "  summing = sum(len(v) for v in data_dict.values())\n",
    "  ##math.ceil to Round up to the next whole number for the product of summing and term.\n",
    "  value_1 =math.ceil(term * summing)\n",
    "  return value_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P-value :\n",
    "p_value = p_value_calc(incomplete_tuples,data_dict)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a dictionary to find out the number of similarity scores associated with each incomplete tuple. We get the length of the dictionary's values for a particular element.\n",
    "def length_getter(list_a,dict_a):\n",
    "  list_length = []\n",
    "  incomplete_tuples = list_a\n",
    "  data_dict1 = dict_a\n",
    "  ## checking if element in incomplete_tuples\n",
    "  for y in incomplete_tuples:\n",
    "    ##for that element we check the similarity scores in data_dict1 dictionary\n",
    "    for y in data_dict1:\n",
    "      ##intializing total to 0\n",
    "      total = 0\n",
    "      #print(\"Element:\",y)\n",
    "      #referencing to fetch the similarity scores -> ex: data_dict1['X1']=[0.814,0.836]\n",
    "      value_list = data_dict1[y]\n",
    "      #print(value_list)\n",
    "      ##Taking their length\n",
    "      count = len(value_list)\n",
    "      total += count\n",
    "      ##appending individual length to a list\n",
    "      list_length.append(total)\n",
    "  #print(list_length)\n",
    "  ##Creating the dictionary which contains the incomplete tuples and their associated value lengths.\n",
    "  zipper=zip(incomplete_tuples,list_length)\n",
    "  data_dict = dict(zipper)\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict2 = length_getter(incomplete_tuples,data_dict1)\n",
    "# print(data_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating the node weights and putting them in a dictionary function\n",
    "def node_weight_calculator(dict_1,dict_2,tuple_1,value1):\n",
    "  list_scores = [] \n",
    "  diction_1 = dict_1\n",
    "  diction_2 = dict_2 \n",
    "  tupled_1 = tuple_1 \n",
    "  p_value = value1\n",
    "  ## iterating over incomplete tuple  present in incomplete_tuples list.\n",
    "  for ij in tupled_1:\n",
    "    #calculating the length of each item in data_dict2\n",
    "    length = (dict_2[ij])\n",
    "    #if length is greater than 0 we calculate the similarity score, else we assign 0\n",
    "    if length>0:\n",
    "      multiply_list = []\n",
    "      #iterate over the length so as to capture all the associate similarity pairs with each incomplete tuple.\n",
    "      for i in range(length):\n",
    "        sim_score = dict_1[ij][i]\n",
    "        ##Calculating the part of node weight from the formula given in the paper.\n",
    "        initial_calc = (1-sim_score)**(1/(p_value))\n",
    "        ##appending to multiply_list the values of initial_calc\n",
    "        multiply_list.append(initial_calc)\n",
    "      #Multiplying all the initial_Calc associated with a element.\n",
    "      weight_1 =np.prod(multiply_list)\n",
    "      ##subtracting 1 from weight_1 as per formula and rounding it to three decimal places to get the final node weight.\n",
    "      final_weight = np.round(1 - weight_1,3)\n",
    "    else:\n",
    "      #keeping node weight as 0 if there is no similarity\n",
    "      final_weight = 0.0\n",
    "    list_scores.append(final_weight)\n",
    "  ##Creating the final dictionary which contains the incomplete tuple and its associated node weight.\n",
    "  zipper_score = zip(tupled_1,list_scores)\n",
    "  dict_output = dict(zipper_score)\n",
    "  return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initial Node weight calculator\n",
    "dict_scores = node_weight_calculator(data_dict1,data_dict2,incomplete_tuples,p_value)\n",
    "print(\"Initial Node weight beofre Imputation Order calc:\\n\",dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Incomplete- Incomplete connections and similarities\n",
    "data_dict_inc,data_dict_inc_1 = incomplete_connection_creator(inc_incomplete,incomplete_tuples)\n",
    "print(\"Incomplete neighbours to incomplete tuples:\\n\",data_dict_inc)\n",
    "print(\"Incomplete neighbours similarity Score:\\n\",data_dict_inc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the Node Weight like we did previously for the values.\n",
    "##Creating a dictionary to find out the number of similarity scores associated with each incomplete tuple. We get the length of the dictionary's values for a particular element.\n",
    "data_dict_2 = length_getter(incomplete_tuples,data_dict_inc_1)\n",
    "print(\"Length of incomplete neighbours:\\n\",data_dict_2)\n",
    "\n",
    "##Appending the node weight we have already calculated for these nodes.\n",
    "print(\"Node Weights:\",dict_scores)\n",
    "score_list = []\n",
    "score_list_name = []\n",
    "removed_item =[]\n",
    "for element in incomplete_tuples:\n",
    "  if data_dict_2[element] > 0:\n",
    "    score_list_name.append(element)\n",
    "    score_list.append(dict_scores[element])    \n",
    "  else:\n",
    "    print(\"Removed\")\n",
    "    removed_item.append(element)\n",
    "zip_score_final = zip(score_list_name,score_list)\n",
    "data_final_scores = dict(zip_score_final)\n",
    "print(\"removed item\",removed_item)\n",
    "print(\"Node Weights input to Greedy Algorithm:\\n\",data_final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incomplete_connection_creator1(list_1,dict_1):\n",
    "  listed = []\n",
    "  listed1= []\n",
    "  inc_complete = list_1\n",
    "  incomplete_tuples = dict_1\n",
    "  #print(inc_complete)\n",
    "  #print(incomplete_tuples)\n",
    "  for column in inc_complete:\n",
    "    for column in incomplete_tuples:\n",
    "      name = column\n",
    "      #print(name)\n",
    "      kailai = (list(inc_complete[inc_complete[column] != 0.0].indexed))\n",
    "      kailai1 = (list(inc_complete[inc_complete[column] != 0.0][column]))\n",
    "      #print(kailai)\n",
    "      listed.append(kailai)\n",
    "      listed1.append(kailai1)\n",
    "    #print(\"listed:\",listed)\n",
    "    zipped = zip(incomplete_tuples,listed)\n",
    "    zipped1 = zip(incomplete_tuples,listed1)\n",
    "    dict_da = dict(zipped)\n",
    "    dict_db = dict(zipped1)\n",
    "    return dict_da,dict_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GREEDY ALGORITHM TO FIND THE IMPUTATION ORDER\n",
    "STEP 1: Find Node with Largest Weight. Keep it as the starting point\n",
    "STEP 2: Calculate the Filling Gain to make it a complete Neighbour.\n",
    "STEP 3: If the imputed Node, contains any connections to incomplete tuples then update their node weights by considering the imputed node\n",
    "        as a completed neighbour.\n",
    "STEP 4: Repeat this process untill all the incomplete tuples are imputed.\n",
    "'''\n",
    "\n",
    "##STEP 1: Find Node with Largest Weight. Keep it as the starting point\n",
    "#Getting the data_final_scores\n",
    "incomplete_tuples_score = data_final_scores\n",
    "print(\"Incomplete Tuple Scores Before Iteration:\",incomplete_tuples_score)\n",
    "#updated_incomplete_tuples = incomplete_tuples.copy()\n",
    "updated_incomplete_tuples = score_list_name.copy()\n",
    "updated_complete_tuples = complete_tuples.copy()\n",
    "final_greedy_order = []\n",
    "print(\"Node Weight for incomplete tuples stored in a new dictionary:\",incomplete_tuples_score)\n",
    "################################################################################################\n",
    "##########################:\n",
    "for i in range(len(incomplete_tuples_score)):  \n",
    "  max_value = max(incomplete_tuples_score, key=incomplete_tuples_score.get)\n",
    "  print(\"Node With the largest weight:\",max_value)\n",
    "  print(\"Node weight:\",incomplete_tuples_score[max_value])\n",
    "  \n",
    "  ##Calculate the Filling Gain\n",
    "  #print(\"X is:\",data_dict_2[max_value])\n",
    "  #print(\"Pairs are :\",data_dict_inc[max_value])\n",
    "\n",
    "  # data_dict_inc.pop('X6',None)\n",
    "  # data_dict_2.pop('X6',None)\n",
    "  # print(data_dict_2)\n",
    "  # print(incomplete_tuples_score)\n",
    "  if max_value in data_dict_2:\n",
    "    list_score_holder = []\n",
    "    final_gain_score_holder = []  \n",
    "    for i in range(data_dict_2[max_value]):\n",
    "      try:\n",
    "       w_label = data_dict_inc[max_value][i]\n",
    "       w_score = incomplete_tuples_score[w_label]\n",
    "       print(\"Connected pair and score :\",w_label,w_score)\n",
    "       list_score_holder.append(w_score)\n",
    "       #print(data_dict_inc_1[max_value][i])\n",
    "       #print(p_value)\n",
    "       w_difference = np.round((incomplete_tuples_score[max_value] - list_score_holder[i]),3)\n",
    "       #print(data_dict_inc_1[max_value][i])\n",
    "       w_second_part =np.round((1-data_dict_inc_1[max_value][i])**(1/p_value),3)\n",
    "       gain = np.round(((w_difference)*(1-w_second_part)),3)\n",
    "       #print(w_difference)\n",
    "       #print(w_second_part)\n",
    "       #print(\"Gain is :\",gain)\n",
    "       final_gain_score_holder.append(gain)\n",
    "       #print(\"Final Gain calculated:\",final_gain_score_holder)\n",
    "      except Exception as e:\n",
    "       print(e)\n",
    "\n",
    "    gain_for_element = sum(final_gain_score_holder)\n",
    "    print(\"Gain For incomplete tuple:\",gain_for_element )\n",
    "    updated_incomplete_tuples.remove(max_value)\n",
    "    updated_complete_tuples.append(max_value)\n",
    "    print(\"Updated_incomplete_tuples\",updated_incomplete_tuples)\n",
    "    print(\"Updated complete tuples\",updated_complete_tuples)\n",
    "    inc_complete_updated = unstacked[~unstacked['indexed'].isin(updated_incomplete_tuples)]\n",
    "    data_dict_upd_1,data_dict_upd_2 = incomplete_connection_creator1(inc_complete_updated,updated_incomplete_tuples)\n",
    "    upd_p_value = p_value_calc(updated_incomplete_tuples,data_dict_upd_1)\n",
    "    data_dict_upd = length_getter(updated_incomplete_tuples,data_dict_upd_2)\n",
    "    print(\"data_dict_upd_2:-\",data_dict_upd_2)\n",
    "    print(\"data_dict_upd_1:\",data_dict_upd_1)\n",
    "    print(max_value)\n",
    "    new_dict = { k : v for k,v in data_dict_upd_1.items() if max_value in v}\n",
    "    new_dict_scores = []\n",
    "    new_dict_key = []\n",
    "    for key,value in new_dict.items():\n",
    "      new_value = data_dict_upd_2[key]\n",
    "      new_dict_scores.append(new_value)\n",
    "      new_key = key\n",
    "      new_dict_key.append(key)\n",
    "    new_zipped = zip(new_dict_key,new_dict_scores)\n",
    "    new_dict_input = dict(new_zipped)\n",
    "    print(new_dict_input)\n",
    "    dict_scores_upd = node_weight_calculator(new_dict_input,data_dict_upd,new_dict_key,p_value)\n",
    "    print(\"Updated Node weight before Imputation Order calc:\\n\",dict_scores_upd)\n",
    "    dict_scores_upd.update({max_value:0.0})\n",
    "    final_greedy_order.append(max_value)\n",
    "    print(\"Appended:\",dict_scores_upd)\n",
    "    incomplete_tuples_score.update(dict_scores_upd)\n",
    "    print(\"Updated Incomplete Tuples scores before proceeding to next iter\",incomplete_tuples_score)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Final Greedy Imputation order is\",final_greedy_order)\n",
    "\n",
    "##Creating the final order for feeding to the KNN\n",
    "final_order = removed_item+final_greedy_order\n",
    "print(\"final_order\",final_order)\n",
    "final_greedy_impute = []\n",
    "# for a in final_order:\n",
    "#   str_name = str(a)\n",
    "#   strip_name = str_name[1:]\n",
    "#   final_greedy_impute.append(int(strip_name))\n",
    "data_top= np.array(data_top)\n",
    "data_top=data_top+1\n",
    "final_greedy_impute= data_top\n",
    "# print(data_top)\n",
    "print(final_greedy_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#KNN Modelling Approach\n",
    "dataset_1 = dataset.copy()\n",
    "# dataset_1 \n",
    "dataset_1.set_index('index',inplace=True)\n",
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Order Imputed Sequential KNN \n",
    "##Creating a null empty array equal to the dataset size\n",
    "dataset_array = dataset_1.to_numpy()\n",
    "print(\"Dataset Converted into a Numpy Array:\\n\",dataset_array)\n",
    "#imputed_data = np.zeros([len(dataset_1),len(dataset_1.columns)])\n",
    "imputed_data = np.zeros_like(dataset_array)\n",
    "print(\"\\nZeroes Array\\n\",imputed_data)\n",
    "#Creating Empty lists \n",
    "complete = []\n",
    "incompleted = []\n",
    "missing =[]\n",
    "com_ind=[]\n",
    "incomp_ind=[]\n",
    "##Taking the rows and cols of the input dataset for iteration purpose\n",
    "[rows,cols] = dataset_array.shape\n",
    "print(\"\\nNo of Rows are\",rows)\n",
    "print(\"No of Cols are\",cols)\n",
    "##iterating through the number of rows\n",
    "for i in range(len(dataset_array)):\n",
    "  a = dataset_array[i]\n",
    "  #print(\"Row is \",a)\n",
    "  nan_checker = np.sum(np.sum(a, axis=0))\n",
    "  #print(\"Sum of Row is\",nan_checker)\n",
    "  if math.isnan(nan_checker) :\n",
    "    #print(\"Spotted Nan Value - \",nan_checker)\n",
    "    incompleted.append(a)\n",
    "    incomp_ind.append(i+1)\n",
    "    #print(\"Non Sum of non numbers\",np.nansum(np.nansum(a, axis=0)))\n",
    "  else:\n",
    "    complete.append(a)\n",
    "    com_ind.append(i+1)\n",
    "zipped_com = zip(com_ind,complete)\n",
    "dict_complete = dict(zipped_com)\n",
    "##Ordering the list as per imputation order generated from Greedy\n",
    "zipped_incom = zip(incomp_ind,incompleted)\n",
    "dict_incompleted = dict(zipped_incom)\n",
    "print(dict_incompleted)\n",
    "sorted_incomplete =[(v,dict_incompleted[v]) for v in final_greedy_impute]\n",
    "print(sorted_incomplete)\n",
    "incom_ind = []\n",
    "incomplete = []\n",
    "for header in range(len(sorted_incomplete)):\n",
    "  header_app = sorted_incomplete[header][0]\n",
    "  value_app = sorted_incomplete[header][1]\n",
    "  incom_ind.append(header_app)\n",
    "  incomplete.append(value_app)\n",
    "print(incom_ind)\n",
    "print(incomplete)\n",
    "incom_ind_1 = sorted_incomplete[0][0]\n",
    "print(incom_ind_1)\n",
    "for i in com_ind:\n",
    "  imputed_data[i-1] = dict_complete[i]\n",
    "print(\"Imputed Data After Filling the Complete Rows:\\n\",imputed_data)\n",
    "print(\"Incomplete Rows in the Array\\n\",incomplete)\n",
    "\n",
    "for k in incomplete:\n",
    "  #print(k)\n",
    "  dist = []\n",
    "  cgen = len(complete)\n",
    "  for j in range(cgen):\n",
    "    #print(k)\n",
    "    #print(\"j\",j)\n",
    "    #print(complete[j])\n",
    "    d = np.nansum(np.power(([k]-complete[j]),2))\n",
    "    dist.append(d)\n",
    "  print(\"Distance Before Sort:\",dist)  \n",
    "  position=[]\n",
    "  for l in range(len(dist)):\n",
    "    position.append(l)\n",
    "  zipped_pos = zip(position,dist)\n",
    "  dict_pos = dict(zipped_pos)\n",
    "  #print(dict_pos)\n",
    "  from operator import itemgetter  \n",
    "  dict_pos_sorted =  sorted(dict_pos.items(), key = itemgetter(1), reverse = False)\n",
    "  print(\"Sorted Dictionary\",dict_pos_sorted)\n",
    "  K = 5\n",
    "  cluster_dictionary = dict_pos_sorted[:5]\n",
    "  print(\"K Cluster Dictionary\",cluster_dictionary)\n",
    "  k_dist = []\n",
    "  k_weight = []\n",
    "  k_final_weight = []\n",
    "  for d,s in cluster_dictionary:\n",
    "    k_dist.append(s)\n",
    "  print(\"Retrieved K Distances\",k_dist)\n",
    "  #total_list_sum = sum(1/((k_dist)))\n",
    "  #print(total_list_sum)\n",
    "  for h in range(len(k_dist)):\n",
    "    #weight = np.true_divide((1/k_dist[h]),)\n",
    "    individual_weight = ((1/k_dist[h]))\n",
    "    k_weight.append(individual_weight)\n",
    "  for u in k_weight:\n",
    "    final_weight = np.true_divide(u,sum(k_weight))\n",
    "    k_final_weight.append(final_weight)\n",
    "  #print(\"K Weight:\",k_weight)   \n",
    "  print(\"Final_Weight:\",k_final_weight)\n",
    "  #print(complete)\n",
    "  for a in range(np.size(dataset_array,1)):\n",
    "      complete_values_list =[] \n",
    "      column_value = k[a]\n",
    "      if math.isnan(column_value):\n",
    "        for o,p in cluster_dictionary:\n",
    "          complete_value = complete[o][a]\n",
    "          complete_values_list.append(complete_value)\n",
    "        #print(\"Complete_value\",complete_values_list)\n",
    "        imputed_final_value = round(sum(f * e for f, e in zip(complete_values_list,k_final_weight)),3)\n",
    "        print(\"-----------------------------------------------------------------------------------------\")\n",
    "        print(\"Final Imputed value:\",imputed_final_value) \n",
    "        print(\"-----------------------------------------------------------------------------------------\")\n",
    "        k[a] = imputed_final_value\n",
    "  complete.append(k)\n",
    "incomplete_dataframe = pd.DataFrame(incomplete)\n",
    "incomplete_dataframe['ind'] = incom_ind\n",
    "incomplete_dataframe.set_index('ind',inplace=True)\n",
    "imputed_data_frame = pd.DataFrame(imputed_data)\n",
    "imputed_data_frame.index = imputed_data_frame.index+1\n",
    "final_imputed_df = incomplete_dataframe.combine_first(imputed_data_frame)\n",
    "final_imputed_df.columns = dataset_1.columns\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Final Imputed Dataset:\\n\", final_imputed_df)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Completed\")\n",
    "final_imputed_df.to_excel('Destination_folder_path_with_file_name_# example\n",
    "/Users/Downloads/imputed_Ionosphere_AE_1.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
